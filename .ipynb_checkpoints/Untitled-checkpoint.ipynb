{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb42494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.21-cp39-cp39-win_amd64.whl (50.9 MB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from mediapipe) (1.4.0)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Collecting opencv-contrib-python\n",
      "  Using cached opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl (46.2 MB)\n",
      "Collecting protobuf<5,>=4.25.3\n",
      "  Using cached protobuf-4.25.6-cp39-cp39-win_amd64.whl (413 kB)\n",
      "Collecting sounddevice>=0.4.4\n",
      "  Using cached sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Collecting jax\n",
      "  Using cached jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mediapipe) (3.5.1)\n",
      "Collecting jaxlib\n",
      "  Using cached jaxlib-0.4.30-cp39-cp39-win_amd64.whl (51.9 MB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jax->mediapipe) (4.11.3)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Collecting ml-dtypes>=0.2.0\n",
      "  Using cached ml_dtypes-0.5.1-cp39-cp39-win_amd64.whl (209 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.7.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Installing collected packages: ml-dtypes, jaxlib, sounddevice, sentencepiece, protobuf, opencv-contrib-python, jax, mediapipe\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "Successfully installed jax-0.4.30 jaxlib-0.4.30 mediapipe-0.10.21 ml-dtypes-0.5.1 opencv-contrib-python-4.11.0.86 protobuf-4.25.6 sentencepiece-0.2.0 sounddevice-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "google-api-core 1.25.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.22.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#pip install mediapipe\n",
    "#pip install mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fabc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.21\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "print(mp.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc306bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to recognize hand gestures\n",
    "def recognize_gesture(hand_landmarks):\n",
    "    landmarks = hand_landmarks.landmark\n",
    "\n",
    "    # Finger landmark indices in MediaPipe\n",
    "    thumb_tip = landmarks[4]\n",
    "    index_tip = landmarks[8]\n",
    "    middle_tip = landmarks[12]\n",
    "    ring_tip = landmarks[16]\n",
    "    pinky_tip = landmarks[20]\n",
    "    wrist = landmarks[0]\n",
    "\n",
    "    # Boolean conditions for finger positions (open or closed)\n",
    "    thumb_open = thumb_tip.x < landmarks[3].x\n",
    "    index_open = index_tip.y < landmarks[6].y\n",
    "    middle_open = middle_tip.y < landmarks[10].y\n",
    "    ring_open = ring_tip.y < landmarks[14].y\n",
    "    pinky_open = pinky_tip.y < landmarks[18].y\n",
    "\n",
    "    # Gesture classification based on finger positions\n",
    "    if thumb_open and not index_open and not middle_open and not ring_open and not pinky_open:\n",
    "        return \"call\"\n",
    "    elif not thumb_open and not index_open and not middle_open and not ring_open and not pinky_open:\n",
    "        return \"fist\"\n",
    "    elif not thumb_open and index_open and not middle_open and not ring_open and not pinky_open:\n",
    "        return \"one\"\n",
    "    elif not thumb_open and index_open and middle_open and not ring_open and not pinky_open:\n",
    "        return \"peace\"\n",
    "    elif not thumb_open and index_open and middle_open and ring_open and pinky_open:\n",
    "        return \"palm\"\n",
    "    elif thumb_open and index_open and middle_open and ring_open and pinky_open:\n",
    "        return \"like\"\n",
    "    elif not thumb_open and index_open and middle_open and not ring_open and pinky_open:\n",
    "        return \"rock\"\n",
    "    elif not thumb_open and index_open and middle_open and ring_open and not pinky_open:\n",
    "        return \"three\"\n",
    "    elif not thumb_open and not index_open and not middle_open and ring_open and pinky_open:\n",
    "        return \"two up\"\n",
    "    elif thumb_open and not index_open and not middle_open and not ring_open and pinky_open:\n",
    "        return \"dislike\"\n",
    "    elif not thumb_open and not index_open and not middle_open and not ring_open and not pinky_open:\n",
    "        return \"stop\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip and process image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            gesture = recognize_gesture(hand_landmarks)\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Display detected gesture\n",
    "            cv2.putText(frame, gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea28cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1b99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mphands = mp.solutions.hands\n",
    "\n",
    "# Initialize hand tracking\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mphands.Hands()\n",
    "\n",
    "def detect_gesture(landmarks):\n",
    "    thumb_tip = landmarks[mphands.HandLandmark.THUMB_TIP]\n",
    "    index_tip = landmarks[mphands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = landmarks[mphands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = landmarks[mphands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = landmarks[mphands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    # Calculate distances between fingertips\n",
    "    def distance(a, b):\n",
    "        return math.sqrt((a.x - b.x)**2 + (a.y - b.y)**2)\n",
    "\n",
    "    # Gesture detection logic\n",
    "    if distance(index_tip, thumb_tip) < 0.05:\n",
    "        return \"ok\"\n",
    "    elif distance(middle_tip, thumb_tip) < 0.05:\n",
    "        return \"fist\"\n",
    "    # Add more gesture detection logic here based on your image\n",
    "\n",
    "    return None\n",
    "\n",
    "while True:\n",
    "    data, image = cap.read()\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mphands.HAND_CONNECTIONS)\n",
    "            \n",
    "            gesture = detect_gesture(hand_landmarks.landmark)\n",
    "            if gesture:\n",
    "                cv2.putText(image, gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Handtracker', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d1180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1eccc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mphands = mp.solutions.hands\n",
    "\n",
    "# Initialize hand tracking\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mphands.Hands()\n",
    "\n",
    "def detect_gesture(landmarks):\n",
    "    thumb_tip = landmarks[mphands.HandLandmark.THUMB_TIP]\n",
    "    index_tip = landmarks[mphands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = landmarks[mphands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = landmarks[mphands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = landmarks[mphands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    # Calculate distances between fingertips\n",
    "    def distance(a, b):\n",
    "        return math.sqrt((a.x - b.x)**2 + (a.y - b.y)**2)\n",
    "\n",
    "    # Gesture detection logic\n",
    "    if distance(index_tip, thumb_tip) < 0.05:\n",
    "        return \"ok\"\n",
    "    elif distance(middle_tip, thumb_tip) < 0.05:\n",
    "        return \"fist\"\n",
    "    elif distance(pinky_tip, thumb_tip) < 0.05 and distance(ring_tip, thumb_tip) < 0.05:\n",
    "        return \"call\"\n",
    "    elif index_tip.y > middle_tip.y and middle_tip.y > ring_tip.y and ring_tip.y > pinky_tip.y:\n",
    "        return \"dislike\"\n",
    "    # Add more gesture detection logic here based on your image\n",
    "\n",
    "    return None\n",
    "\n",
    "while True:\n",
    "    data, image = cap.read()\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mphands.HAND_CONNECTIONS)\n",
    "            \n",
    "            gesture = detect_gesture(hand_landmarks.landmark)\n",
    "            if gesture:\n",
    "                cv2.putText(image, gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Handtracker', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c583ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fdcc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mphands = mp.solutions.hands\n",
    "\n",
    "# Initialize hand tracking\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mphands.Hands()\n",
    "\n",
    "def detect_gesture(landmarks):\n",
    "    thumb_tip = landmarks[mphands.HandLandmark.THUMB_TIP]\n",
    "    index_tip = landmarks[mphands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = landmarks[mphands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_tip = landmarks[mphands.HandLandmark.RING_FINGER_TIP]\n",
    "    pinky_tip = landmarks[mphands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    # Calculate distances between fingertips\n",
    "    def distance(a, b):\n",
    "        return math.sqrt((a.x - b.x)**2 + (a.y - b.y)**2)\n",
    "\n",
    "    # Gesture detection logic\n",
    "    if distance(index_tip, thumb_tip) < 0.05:\n",
    "        return \"ok\"\n",
    "    elif distance(middle_tip, thumb_tip) < 0.05:\n",
    "        return \"fist\"\n",
    "    elif distance(pinky_tip, thumb_tip) < 0.05 and distance(ring_tip, thumb_tip) < 0.05:\n",
    "        return \"call\"\n",
    "    elif index_tip.y > middle_tip.y and middle_tip.y > ring_tip.y and ring_tip.y > pinky_tip.y:\n",
    "        return \"dislike\"\n",
    "    elif distance(thumb_tip, index_tip) > 0.1 and distance(thumb_tip, middle_tip) > 0.1 and distance(thumb_tip, ring_tip) > 0.1 and distance(thumb_tip, pinky_tip) > 0.1:\n",
    "        return \"four\"\n",
    "    elif thumb_tip.y < index_tip.y and thumb_tip.y < middle_tip.y and thumb_tip.y < ring_tip.y and thumb_tip.y < pinky_tip.y:\n",
    "        return \"like\"\n",
    "    elif distance(index_tip, thumb_tip) < 0.05 and distance(middle_tip, thumb_tip) < 0.05 and distance(ring_tip, thumb_tip) < 0.05 and distance(pinky_tip, thumb_tip) < 0.05:\n",
    "        return \"mute\"\n",
    "    # Add more gesture detection logic here based on your image\n",
    "\n",
    "    return None\n",
    "\n",
    "while True:\n",
    "    data, image = cap.read()\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mphands.HAND_CONNECTIONS)\n",
    "            \n",
    "            gesture = detect_gesture(hand_landmarks.landmark)\n",
    "            if gesture:\n",
    "                cv2.putText(image, gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Handtracker', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f03252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b4fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ece7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mphands = mp.solutions.hands\n",
    "\n",
    "def recognize_gesture(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Recognize hand gestures based on finger positions\n",
    "    \"\"\"\n",
    "    # Get hand landmarks\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    index_tip = hand_landmarks.landmark[8]\n",
    "    middle_tip = hand_landmarks.landmark[12]\n",
    "    ring_tip = hand_landmarks.landmark[16]\n",
    "    pinky_tip = hand_landmarks.landmark[20]\n",
    "    \n",
    "    # Get pip joints\n",
    "    thumb_ip = hand_landmarks.landmark[3]\n",
    "    index_pip = hand_landmarks.landmark[6]\n",
    "    middle_pip = hand_landmarks.landmark[10]\n",
    "    ring_pip = hand_landmarks.landmark[14]\n",
    "    pinky_pip = hand_landmarks.landmark[18]\n",
    "    \n",
    "    # Get mcp joints\n",
    "    index_mcp = hand_landmarks.landmark[5]\n",
    "    middle_mcp = hand_landmarks.landmark[9]\n",
    "    ring_mcp = hand_landmarks.landmark[13]\n",
    "    pinky_mcp = hand_landmarks.landmark[17]\n",
    "    \n",
    "    wrist = hand_landmarks.landmark[0]\n",
    "    \n",
    "    # Check if fingers are extended\n",
    "    # For thumb, consider its position relative to the wrist\n",
    "    is_thumb_extended = thumb_tip.x > thumb_ip.x if wrist.x < thumb_ip.x else thumb_tip.x < thumb_ip.x\n",
    "    is_index_extended = index_tip.y < index_pip.y\n",
    "    is_middle_extended = middle_tip.y < middle_pip.y\n",
    "    is_ring_extended = ring_tip.y < ring_pip.y\n",
    "    is_pinky_extended = pinky_tip.y < pinky_pip.y\n",
    "    \n",
    "    # Create a list representing finger state (True = extended, False = folded)\n",
    "    fingers = [is_thumb_extended, is_index_extended, is_middle_extended, is_ring_extended, is_pinky_extended]\n",
    "    \n",
    "    # Calculate distances between fingertips\n",
    "    thumb_index_distance = math.sqrt((thumb_tip.x - index_tip.x)**2 + (thumb_tip.y - index_tip.y)**2)\n",
    "    \n",
    "    # Check for specific gestures\n",
    "    \n",
    "    # Fist - no fingers extended\n",
    "    if not any(fingers):\n",
    "        return \"fist\"\n",
    "    \n",
    "    # One - only index finger extended\n",
    "    elif fingers == [False, True, False, False, False]:\n",
    "        return \"one\"\n",
    "    \n",
    "    # Peace and Two up - index and middle extended\n",
    "    elif fingers == [False, True, True, False, False]:\n",
    "        # Calculate angle between index and middle fingers\n",
    "        v1 = [index_tip.x - index_mcp.x, index_tip.y - index_mcp.y]\n",
    "        v2 = [middle_tip.x - middle_mcp.x, middle_tip.y - middle_mcp.y]\n",
    "        dot_product = v1[0] * v2[0] + v1[1] * v2[1]\n",
    "        mag1 = math.sqrt(v1[0]**2 + v1[1]**2)\n",
    "        mag2 = math.sqrt(v2[0]**2 + v2[1]**2)\n",
    "        angle = math.acos(max(-1, min(1, dot_product / (mag1 * mag2)))) * 180 / math.pi\n",
    "        \n",
    "        if index_tip.y < index_mcp.y:  # Fingers pointing up\n",
    "            if angle < 30:  # Fingers close together\n",
    "                return \"two up\"\n",
    "            else:\n",
    "                return \"peace\"\n",
    "        else:  # Fingers pointing down\n",
    "            if angle < 30:\n",
    "                return \"two up inv.\"\n",
    "            else:\n",
    "                return \"peace inv.\"\n",
    "    \n",
    "    # Three - index, middle, and ring extended\n",
    "    elif fingers == [False, True, True, True, False]:\n",
    "        return \"three\"\n",
    "    \n",
    "    # Three 2 - thumb, index, and pinky extended\n",
    "    elif fingers == [True, True, False, False, True]:\n",
    "        return \"three 2\"\n",
    "    \n",
    "    # Four - all fingers except thumb extended\n",
    "    elif fingers == [False, True, True, True, True]:\n",
    "        return \"four\"\n",
    "    \n",
    "    # Five/Palm - all fingers extended\n",
    "    elif all(fingers):\n",
    "        return \"palm\"\n",
    "    \n",
    "    # OK sign - thumb and index forming a circle\n",
    "    elif thumb_index_distance < 0.1:\n",
    "        return \"ok\"\n",
    "    \n",
    "    # Like (thumbs up) or Dislike (thumbs down)\n",
    "    elif fingers == [True, False, False, False, False]:\n",
    "        return \"like\" if thumb_tip.y < wrist.y else \"dislike\"\n",
    "    \n",
    "    # Rock - index and pinky extended\n",
    "    elif not fingers[0] and fingers[1] and not fingers[2] and not fingers[3] and fingers[4]:\n",
    "        return \"rock\"\n",
    "    \n",
    "    # Stop/Stop inv. - all fingers except thumb extended, palm orientation\n",
    "    elif all(fingers[1:]):\n",
    "        # Simplified check for palm orientation\n",
    "        if index_tip.z < index_mcp.z:  # Palm facing camera\n",
    "            return \"stop\"\n",
    "        else:\n",
    "            return \"stop inv.\"\n",
    "    \n",
    "    # Call (telephone gesture) - thumb and pinky extended\n",
    "    elif fingers == [True, False, False, False, True]:\n",
    "        return \"call\"\n",
    "    \n",
    "    # Mute - index finger near mouth\n",
    "    elif fingers == [False, True, False, False, False]:\n",
    "        # This is an approximation - would need facial landmarks for exact detection\n",
    "        return \"mute\"\n",
    "    \n",
    "    # If no gesture is recognized\n",
    "    return \"Unknown\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mphands.Hands()\n",
    "\n",
    "while True:\n",
    "    data, image = cap.read()\n",
    "    # Flip the image\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # Storing the results\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mphands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Recognize gesture\n",
    "            gesture = recognize_gesture(hand_landmarks)\n",
    "            \n",
    "            # Display gesture name\n",
    "            cv2.putText(image, f\"Gesture: {gesture}\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Handtracker', image)\n",
    "    # Exit loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452a3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mphands = mp.solutions.hands\n",
    "\n",
    "def calculate_finger_angle(landmark1, landmark2, landmark3):\n",
    "    \"\"\"Calculate angle between three points\"\"\"\n",
    "    v1 = [landmark1.x - landmark2.x, landmark1.y - landmark2.y]\n",
    "    v2 = [landmark3.x - landmark2.x, landmark3.y - landmark2.y]\n",
    "    \n",
    "    # Normalize vectors\n",
    "    v1_norm = math.sqrt(v1[0]**2 + v1[1]**2)\n",
    "    v2_norm = math.sqrt(v2[0]**2 + v2[1]**2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if v1_norm == 0 or v2_norm == 0:\n",
    "        return 0\n",
    "    \n",
    "    dot_product = v1[0]*v2[0] + v1[1]*v2[1]\n",
    "    angle_rad = math.acos(min(1, max(-1, dot_product/(v1_norm*v2_norm))))\n",
    "    angle_deg = angle_rad * 180 / math.pi\n",
    "    \n",
    "    return angle_deg\n",
    "\n",
    "def palm_normal(hand_landmarks):\n",
    "    \"\"\"Calculate palm normal vector (perpendicular to palm)\"\"\"\n",
    "    # Get wrist and positions from index and pinky base to form a plane\n",
    "    wrist = np.array([hand_landmarks.landmark[0].x, hand_landmarks.landmark[0].y, hand_landmarks.landmark[0].z])\n",
    "    index_mcp = np.array([hand_landmarks.landmark[5].x, hand_landmarks.landmark[5].y, hand_landmarks.landmark[5].z])\n",
    "    pinky_mcp = np.array([hand_landmarks.landmark[17].x, hand_landmarks.landmark[17].y, hand_landmarks.landmark[17].z])\n",
    "    \n",
    "    # Calculate vectors in the palm plane\n",
    "    v1 = index_mcp - wrist\n",
    "    v2 = pinky_mcp - wrist\n",
    "    \n",
    "    # Normal vector is the cross product\n",
    "    normal = np.cross(v1, v2)\n",
    "    \n",
    "    # Normalize the vector\n",
    "    norm = np.linalg.norm(normal)\n",
    "    if norm == 0:\n",
    "        return np.array([0, 0, 1])  # Default normal if calculation fails\n",
    "    return normal / norm\n",
    "\n",
    "def recognize_gesture(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Recognize hand gestures based on finger positions\n",
    "    \"\"\"\n",
    "    # Get fingertips\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    index_tip = hand_landmarks.landmark[8]\n",
    "    middle_tip = hand_landmarks.landmark[12]\n",
    "    ring_tip = hand_landmarks.landmark[16]\n",
    "    pinky_tip = hand_landmarks.landmark[20]\n",
    "    \n",
    "    # Get finger bases and joints\n",
    "    thumb_ip = hand_landmarks.landmark[3]  # IP joint\n",
    "    thumb_cmc = hand_landmarks.landmark[1]  # CMC joint\n",
    "    \n",
    "    index_pip = hand_landmarks.landmark[6]  # PIP joint\n",
    "    index_mcp = hand_landmarks.landmark[5]  # MCP joint\n",
    "    \n",
    "    middle_pip = hand_landmarks.landmark[10]\n",
    "    middle_mcp = hand_landmarks.landmark[9]\n",
    "    \n",
    "    ring_pip = hand_landmarks.landmark[14]\n",
    "    ring_mcp = hand_landmarks.landmark[13]\n",
    "    \n",
    "    pinky_pip = hand_landmarks.landmark[18]\n",
    "    pinky_mcp = hand_landmarks.landmark[17]\n",
    "    \n",
    "    wrist = hand_landmarks.landmark[0]\n",
    "    \n",
    "    # Check for thumb extension - more reliable method\n",
    "    thumb_angle = calculate_finger_angle(thumb_cmc, thumb_ip, thumb_tip)\n",
    "    is_thumb_extended = thumb_angle > 150\n",
    "    \n",
    "    # Check for finger extension\n",
    "    is_index_extended = index_tip.y < index_pip.y\n",
    "    is_middle_extended = middle_tip.y < middle_pip.y\n",
    "    is_ring_extended = ring_tip.y < ring_pip.y\n",
    "    is_pinky_extended = pinky_tip.y < pinky_pip.y\n",
    "    \n",
    "    # Create a list representing finger state (True = extended, False = folded)\n",
    "    fingers = [is_thumb_extended, is_index_extended, is_middle_extended, is_ring_extended, is_pinky_extended]\n",
    "    \n",
    "    # Calculate distance between thumb and index fingertip (for OK gesture)\n",
    "    thumb_index_distance = math.sqrt((thumb_tip.x - index_tip.x)**2 + (thumb_tip.y - index_tip.y)**2)\n",
    "    \n",
    "    # Check for specific gestures based on the image\n",
    "    \n",
    "    # MUTE GESTURE - Improved detection\n",
    "    # In the image, mute shows index finger near lips with other fingers folded\n",
    "    # We'll detect if only index is up and thumb may be slightly extended\n",
    "    if is_index_extended and not is_middle_extended and not is_ring_extended and not is_pinky_extended:\n",
    "        # Check if index finger is oriented vertically\n",
    "        if abs(index_tip.x - index_mcp.x) < 0.05:  # Index finger relatively straight up\n",
    "            return \"mute\"\n",
    "        elif is_index_extended and not any(fingers[2:]):\n",
    "            return \"one\"\n",
    "    \n",
    "    # STOP GESTURE - Improved detection\n",
    "    # All fingers extended, palm facing camera\n",
    "    if all(fingers[1:]):  # All fingers except thumb extended\n",
    "        # Get palm normal vector\n",
    "        normal = palm_normal(hand_landmarks)\n",
    "        \n",
    "        # Check palm orientation - if z component is positive, palm faces camera\n",
    "        if normal[2] > 0:\n",
    "            return \"stop\"\n",
    "        else:\n",
    "            return \"stop inv.\"\n",
    "    \n",
    "    # Other gestures from the original implementation\n",
    "    \n",
    "    # Fist - no fingers extended\n",
    "    if not any(fingers):\n",
    "        return \"fist\"\n",
    "    \n",
    "    # Peace and Two up - index and middle extended\n",
    "    elif fingers == [False, True, True, False, False]:\n",
    "        # Calculate angle between index and middle fingers\n",
    "        v1 = [index_tip.x - index_mcp.x, index_tip.y - index_mcp.y]\n",
    "        v2 = [middle_tip.x - middle_mcp.x, middle_tip.y - middle_mcp.y]\n",
    "        dot_product = v1[0] * v2[0] + v1[1] * v2[1]\n",
    "        mag1 = math.sqrt(v1[0]**2 + v1[1]**2)\n",
    "        mag2 = math.sqrt(v2[0]**2 + v2[1]**2)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if mag1 == 0 or mag2 == 0:\n",
    "            return \"unknown\"\n",
    "            \n",
    "        angle = math.acos(max(-1, min(1, dot_product / (mag1 * mag2)))) * 180 / math.pi\n",
    "        \n",
    "        if index_tip.y < index_mcp.y:  # Fingers pointing up\n",
    "            if angle < 30:  # Fingers close together\n",
    "                return \"two up\"\n",
    "            else:\n",
    "                return \"peace\"\n",
    "        else:  # Fingers pointing down\n",
    "            if angle < 30:\n",
    "                return \"two up inv.\"\n",
    "            else:\n",
    "                return \"peace inv.\"\n",
    "    \n",
    "    # Three - index, middle, and ring extended\n",
    "    elif fingers == [False, True, True, True, False]:\n",
    "        return \"three\"\n",
    "    \n",
    "    # Three 2 - thumb, index, and pinky extended\n",
    "    elif fingers == [True, True, False, False, True]:\n",
    "        return \"three 2\"\n",
    "    \n",
    "    # Four - all fingers except thumb extended\n",
    "    elif fingers == [False, True, True, True, True]:\n",
    "        return \"four\"\n",
    "    \n",
    "    # Five/Palm - all fingers extended\n",
    "    elif all(fingers):\n",
    "        return \"palm\"\n",
    "    \n",
    "    # OK sign - thumb and index forming a circle\n",
    "    elif thumb_index_distance < 0.1:\n",
    "        return \"ok\"\n",
    "    \n",
    "    # Like (thumbs up) or Dislike (thumbs down)\n",
    "    elif fingers[0] and not any(fingers[1:]):\n",
    "        # Check vertical orientation of thumb\n",
    "        return \"like\" if thumb_tip.y < wrist.y else \"dislike\"\n",
    "    \n",
    "    # Rock - index and pinky extended\n",
    "    elif not fingers[0] and fingers[1] and not fingers[2] and not fingers[3] and fingers[4]:\n",
    "        return \"rock\"\n",
    "    \n",
    "    # Call (telephone gesture) - thumb and pinky extended\n",
    "    elif fingers[0] and not fingers[1] and not fingers[2] and not fingers[3] and fingers[4]:\n",
    "        return \"call\"\n",
    "    \n",
    "    # If no gesture is recognized\n",
    "    return \"Unknown\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mphands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    data, image = cap.read()\n",
    "    # Flip the image\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # Process with MediaPipe\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mphands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Recognize gesture\n",
    "            gesture = recognize_gesture(hand_landmarks)\n",
    "            \n",
    "            # Display gesture name\n",
    "            cv2.putText(image, f\"Gesture: {gesture}\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Handtracker', image)\n",
    "    # Exit loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c144c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f95403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mphands = mp.solutions.hands\n",
    "\n",
    "def calculate_finger_angle(landmark1, landmark2, landmark3):\n",
    "    \"\"\"Calculate angle between three points\"\"\"\n",
    "    v1 = [landmark1.x - landmark2.x, landmark1.y - landmark2.y]\n",
    "    v2 = [landmark3.x - landmark2.x, landmark3.y - landmark2.y]\n",
    "    \n",
    "    # Normalize vectors\n",
    "    v1_norm = math.sqrt(v1[0]**2 + v1[1]**2)\n",
    "    v2_norm = math.sqrt(v2[0]**2 + v2[1]**2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if v1_norm == 0 or v2_norm == 0:\n",
    "        return 0\n",
    "    \n",
    "    dot_product = v1[0]*v2[0] + v1[1]*v2[1]\n",
    "    angle_rad = math.acos(min(1, max(-1, dot_product/(v1_norm*v2_norm))))\n",
    "    angle_deg = angle_rad * 180 / math.pi\n",
    "    \n",
    "    return angle_deg\n",
    "\n",
    "def palm_normal(hand_landmarks):\n",
    "    \"\"\"Calculate palm normal vector (perpendicular to palm)\"\"\"\n",
    "    # Get wrist and positions from index and pinky base to form a plane\n",
    "    wrist = np.array([hand_landmarks.landmark[0].x, hand_landmarks.landmark[0].y, hand_landmarks.landmark[0].z])\n",
    "    index_mcp = np.array([hand_landmarks.landmark[5].x, hand_landmarks.landmark[5].y, hand_landmarks.landmark[5].z])\n",
    "    pinky_mcp = np.array([hand_landmarks.landmark[17].x, hand_landmarks.landmark[17].y, hand_landmarks.landmark[17].z])\n",
    "    \n",
    "    # Calculate vectors in the palm plane\n",
    "    v1 = index_mcp - wrist\n",
    "    v2 = pinky_mcp - wrist\n",
    "    \n",
    "    # Normal vector is the cross product\n",
    "    normal = np.cross(v1, v2)\n",
    "    \n",
    "    # Normalize the vector\n",
    "    norm = np.linalg.norm(normal)\n",
    "    if norm == 0:\n",
    "        return np.array([0, 0, 1])  # Default normal if calculation fails\n",
    "    return normal / norm\n",
    "\n",
    "def finger_spread(hand_landmarks):\n",
    "    \"\"\"Check if fingers are spread out\"\"\"\n",
    "    index_tip = hand_landmarks.landmark[8]\n",
    "    middle_tip = hand_landmarks.landmark[12]\n",
    "    ring_tip = hand_landmarks.landmark[16]\n",
    "    pinky_tip = hand_landmarks.landmark[20]\n",
    "    \n",
    "    # Calculate distances between adjacent fingertips\n",
    "    distances = [\n",
    "        math.sqrt((index_tip.x - middle_tip.x)**2 + (index_tip.y - middle_tip.y)**2),\n",
    "        math.sqrt((middle_tip.x - ring_tip.x)**2 + (middle_tip.y - ring_tip.y)**2),\n",
    "        math.sqrt((ring_tip.x - pinky_tip.x)**2 + (ring_tip.y - pinky_tip.y)**2)\n",
    "    ]\n",
    "    \n",
    "    # If fingers are spread, distances should be significant\n",
    "    return min(distances) > 0.04  # Adjust threshold as needed\n",
    "\n",
    "def recognize_gesture(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Recognize hand gestures based on finger positions\n",
    "    \"\"\"\n",
    "    # Get fingertips\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    index_tip = hand_landmarks.landmark[8]\n",
    "    middle_tip = hand_landmarks.landmark[12]\n",
    "    ring_tip = hand_landmarks.landmark[16]\n",
    "    pinky_tip = hand_landmarks.landmark[20]\n",
    "    \n",
    "    # Get finger bases and joints\n",
    "    thumb_ip = hand_landmarks.landmark[3]  # IP joint\n",
    "    thumb_cmc = hand_landmarks.landmark[1]  # CMC joint\n",
    "    \n",
    "    index_pip = hand_landmarks.landmark[6]  # PIP joint\n",
    "    index_mcp = hand_landmarks.landmark[5]  # MCP joint\n",
    "    \n",
    "    middle_pip = hand_landmarks.landmark[10]\n",
    "    middle_mcp = hand_landmarks.landmark[9]\n",
    "    \n",
    "    ring_pip = hand_landmarks.landmark[14]\n",
    "    ring_mcp = hand_landmarks.landmark[13]\n",
    "    \n",
    "    pinky_pip = hand_landmarks.landmark[18]\n",
    "    pinky_mcp = hand_landmarks.landmark[17]\n",
    "    \n",
    "    wrist = hand_landmarks.landmark[0]\n",
    "    \n",
    "    # Check for thumb extension - more reliable method\n",
    "    thumb_angle = calculate_finger_angle(thumb_cmc, thumb_ip, thumb_tip)\n",
    "    is_thumb_extended = thumb_angle > 150\n",
    "    \n",
    "    # Check for finger extension\n",
    "    is_index_extended = index_tip.y < index_pip.y\n",
    "    is_middle_extended = middle_tip.y < middle_pip.y\n",
    "    is_ring_extended = ring_tip.y < ring_pip.y\n",
    "    is_pinky_extended = pinky_tip.y < pinky_pip.y\n",
    "    \n",
    "    # Create a list representing finger state (True = extended, False = folded)\n",
    "    fingers = [is_thumb_extended, is_index_extended, is_middle_extended, is_ring_extended, is_pinky_extended]\n",
    "    \n",
    "    # Calculate distance between thumb and index fingertip (for OK gesture)\n",
    "    thumb_index_distance = math.sqrt((thumb_tip.x - index_tip.x)**2 + (thumb_tip.y - index_tip.y)**2)\n",
    "    \n",
    "    # PALM DETECTION - Fixed Implementation\n",
    "    # For palm: all fingers extended AND spread apart AND palm facing camera\n",
    "    if all(fingers):\n",
    "        # Check if fingers are spread\n",
    "        are_fingers_spread = finger_spread(hand_landmarks)\n",
    "        \n",
    "        # Get palm normal and check orientation\n",
    "        normal = palm_normal(hand_landmarks)\n",
    "        palm_facing_camera = normal[2] > 0\n",
    "        \n",
    "        # Additional check: fingers should be relatively aligned (not bent at odd angles)\n",
    "        finger_alignment = True\n",
    "        for i in range(1, 5):  # Check index through pinky\n",
    "            # Get the finger's joints\n",
    "            tip = hand_landmarks.landmark[4*i+4]\n",
    "            pip = hand_landmarks.landmark[4*i+2]\n",
    "            mcp = hand_landmarks.landmark[4*i+1]\n",
    "            \n",
    "            # Check if finger is straight\n",
    "            angle = calculate_finger_angle(tip, pip, mcp)\n",
    "            if angle < 160:  # Adjust threshold as needed\n",
    "                finger_alignment = False\n",
    "                break\n",
    "        \n",
    "        # For palm: fingers spread AND facing camera AND fingers aligned\n",
    "        if are_fingers_spread and palm_facing_camera and finger_alignment:\n",
    "            return \"palm\"\n",
    "    \n",
    "    # MUTE GESTURE - Improved detection\n",
    "    if is_index_extended and not is_middle_extended and not is_ring_extended and not is_pinky_extended:\n",
    "        # Check if index finger is oriented vertically\n",
    "        if abs(index_tip.x - index_mcp.x) < 0.05:  # Index finger relatively straight up\n",
    "            return \"mute\"\n",
    "        elif is_index_extended and not any(fingers[2:]):\n",
    "            return \"one\"\n",
    "    \n",
    "    # STOP GESTURE - Improved detection\n",
    "    if all(fingers[1:]):  # All fingers except thumb extended\n",
    "        # Get palm normal vector\n",
    "        normal = palm_normal(hand_landmarks)\n",
    "        \n",
    "        # Check palm orientation - if z component is positive, palm faces camera\n",
    "        if normal[2] > 0:\n",
    "            return \"stop\"\n",
    "        else:\n",
    "            return \"stop inv.\"\n",
    "    \n",
    "    # Other gestures from the original implementation\n",
    "    \n",
    "    # Fist - no fingers extended\n",
    "    if not any(fingers):\n",
    "        return \"fist\"\n",
    "    \n",
    "    # Peace and Two up - index and middle extended\n",
    "    elif fingers == [False, True, True, False, False]:\n",
    "        # Calculate angle between index and middle fingers\n",
    "        v1 = [index_tip.x - index_mcp.x, index_tip.y - index_mcp.y]\n",
    "        v2 = [middle_tip.x - middle_mcp.x, middle_tip.y - middle_mcp.y]\n",
    "        dot_product = v1[0] * v2[0] + v1[1] * v2[1]\n",
    "        mag1 = math.sqrt(v1[0]**2 + v1[1]**2)\n",
    "        mag2 = math.sqrt(v2[0]**2 + v2[1]**2)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if mag1 == 0 or mag2 == 0:\n",
    "            return \"unknown\"\n",
    "            \n",
    "        angle = math.acos(max(-1, min(1, dot_product / (mag1 * mag2)))) * 180 / math.pi\n",
    "        \n",
    "        if index_tip.y < index_mcp.y:  # Fingers pointing up\n",
    "            if angle < 30:  # Fingers close together\n",
    "                return \"two up\"\n",
    "            else:\n",
    "                return \"peace\"\n",
    "        else:  # Fingers pointing down\n",
    "            if angle < 30:\n",
    "                return \"two up inv.\"\n",
    "            else:\n",
    "                return \"peace inv.\"\n",
    "    \n",
    "    # Three - index, middle, and ring extended\n",
    "    elif fingers == [False, True, True, True, False]:\n",
    "        return \"three\"\n",
    "    \n",
    "    # Three 2 - thumb, index, and pinky extended\n",
    "    elif fingers == [True, True, False, False, True]:\n",
    "        return \"three 2\"\n",
    "    \n",
    "    # Four - all fingers except thumb extended\n",
    "    elif fingers == [False, True, True, True, True]:\n",
    "        return \"four\"\n",
    "    \n",
    "    # OK sign - thumb and index forming a circle\n",
    "    elif thumb_index_distance < 0.1:\n",
    "        return \"ok\"\n",
    "    \n",
    "    # Like (thumbs up) or Dislike (thumbs down)\n",
    "    elif fingers[0] and not any(fingers[1:]):\n",
    "        # Check vertical orientation of thumb\n",
    "        return \"like\" if thumb_tip.y < wrist.y else \"dislike\"\n",
    "    \n",
    "    # Rock - index and pinky extended\n",
    "    elif not fingers[0] and fingers[1] and not fingers[2] and not fingers[3] and fingers[4]:\n",
    "        return \"rock\"\n",
    "    \n",
    "    # Call (telephone gesture) - thumb and pinky extended\n",
    "    elif fingers[0] and not fingers[1] and not fingers[2] and not fingers[3] and fingers[4]:\n",
    "        return \"call\"\n",
    "    \n",
    "    # If no gesture is recognized\n",
    "    return \"Unknown\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Increase detection confidence for more stable results\n",
    "hands = mphands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    data, image = cap.read()\n",
    "    # Flip the image\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # Process with MediaPipe\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mphands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Recognize gesture\n",
    "            gesture = recognize_gesture(hand_landmarks)\n",
    "            \n",
    "            # Display gesture name\n",
    "            cv2.putText(image, f\"Gesture: {gesture}\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Handtracker', image)\n",
    "    # Exit loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b23e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
